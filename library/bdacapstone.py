# -*- coding: utf-8 -*-
"""BDACapstone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EhQLiQhUFDjZVnPBpSQCTHHbu_tkYVJi
"""

# imports
import json
import time
import sys
import csv
import glob
import pandas as pd
import numpy as np
import qrcode

import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
from spotipy.oauth2 import SpotifyOAuth

import json
import pandas as pd
import numpy as np
import time
import math
import re
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler

import math

import numpy as np
import numpy.linalg as nla
import pandas as pd
import seaborn as sns
import altair as alt
import re
import pdb  # for Python debugger
import sys
from os.path import join

import matplotlib.pyplot as plt

def read_dump_pickle(path,
                     read=True,
                     dump=False,):
    import pickle

    if read:
        with open(path, 'rb') as handle:
            model = pickle.load(handle)
            print(f"Pickle read from {path}.")
        return model
    elif dump:
        with open(path, 'wb') as handle:
            pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)
        print(f"Pickle dumped under {path}.")
    else:
        print("Something is wrong with the arguments.")

def all_unique_tracks(df,
                      return_only_unique=True):
    track_frames = []
    
    import time
    
    for row in range(len(df)):
        temp_df = pd.DataFrame(df['tracks'][row])
        track_frames.append(temp_df)
    
    if return_only_unique:
        df_tracks = pd.concat(track_frames, axis=0, ignore_index=True).drop_duplicates().reset_index(drop=True)
        all_unique_tracks = list(set(df_tracks['track_uri'].tolist()))
        return all_unique_tracks
    else:
        df_tracks = pd.concat(track_frames, axis=0, ignore_index=True).reset_index(drop=True)
        return df_tracks

def retrieve_tracks_and_features(all_unique_tracks,
                                 connection):
    
    if type(all_unique_tracks) != list:
        all_unique_tracks = [all_unique_tracks]

    sp = connection
    counter = 0
    
    track_features_frames = []
    bulk_problematic_uris = []
    
    if len(all_unique_tracks) > 50:
        chunked_track_uris = np.array_split(all_unique_tracks, int(len(all_unique_tracks)/50))
        
        for track_uris in chunked_track_uris:

            time.sleep(0.05)

            temp_track_dict = sp.audio_features(track_uris)


            if None not in temp_track_dict:
                temp_track_df = pd.DataFrame(temp_track_dict)
              
                track_features_frames.append(temp_track_df)
            else:
                bulk_problematic_uris.extend(track_uris)

            counter += 1
            if counter % 200 == 0:
                print(f'{counter} / {len(chunked_track_uris)}')


        problematic_uris = [] 
        for problem_uri in bulk_problematic_uris:
            time.sleep(0.05)
            temp_track_dict = sp.audio_features(problem_uri)
            if None not in temp_track_dict:
                temp_track_df = pd.DataFrame(temp_track_dict)
              
                track_features_frames.append(temp_track_df)
            else:
                problematic_uris.append(problem_uri)


        df_track_features = pd.concat(track_features_frames, axis=0, ignore_index=True).drop_duplicates().reset_index(drop=True)

        
        return df_track_features, problematic_uris

    else:
        problematic_uris = [] 
        for track_uris in all_unique_tracks:

            time.sleep(0.05)

            temp_track_dict = sp.audio_features(all_unique_tracks)


            if None not in temp_track_dict:
                temp_track_df = pd.DataFrame(temp_track_dict)
              
                track_features_frames.append(temp_track_df)
            else:
                problematic_uris.append(track_uris)

            counter += 1
            print(f'{counter} / {len(all_unique_tracks)}')



        df_track_features = pd.concat(track_features_frames, axis=0, ignore_index=True).drop_duplicates().reset_index(drop=True)

        
        return df_track_features, problematic_uris

def load_track_features(path=r'/content/drive/MyDrive/BDA_CAPSTONE/01_Gathering_Data/01_SpotifyMillionPlaylistDataset/01_Dataset/all_unique_tracks/track_features.csv',
                        drop_cols = ['type', 'id', 'track_href', 'analysis_url', 'Unnamed: 0'],
                        scaled=True):

    
    df_all_track_features = pd.read_csv(path)
    track_cols = df_all_track_features.columns.tolist()

    drops = []
    for drop_col in drop_cols:
        if drop_col in track_cols:
            drops.append(drop_col)

    df_all_track_features.drop(drops, axis=1, inplace=True)

    if scaled:
        from sklearn.preprocessing import MinMaxScaler, StandardScaler
        
        df = df_all_track_features.set_index('uri')
        mms = MinMaxScaler()
        df_scaled = mms.fit_transform(df)
        df_scaled = pd.DataFrame(df_scaled, columns=df.columns, index=df.index)

        return df_scaled
    else:
        return df_all_track_features

def load_clustered_dataset(clustered_dataset_path='/content/drive/MyDrive/BDA_CAPSTONE/01_Gathering_Data/03_Processed_Data/df_scaled_clustered.csv'):
    clustered_dataset_path = clustered_dataset_path

    clustered_dataset = pd.read_csv(clustered_dataset_path).set_index(['uri', 'first_clusters', 'second_clusters'])

    return clustered_dataset

def content_based_recommend(df,
                            array=[1,1,1,1,1,1,1,1,1,1,1,1,1],  # 13 elements
                            track_uri='spotify:track:75L6ZI3hbF3vbkOyBlJup8',
                            is_track_uri=True,
                            n_recommendations=500,
                            similarities=['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan',
                                          'braycurtis', 'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 
                                           'jaccard', 'kulsinski', 'minkowski', 'rogerstanimoto', 
                                           'russellrao', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'],  # more will be added soon. 'mahalanobis', 'seuclidean' are problematic.
                            is_scale = False,
                            scaling='standard',
                            concat_distances=False,  # not yet implemented.
                            return_df=False,
                            write_results=True,
                            verbose=False):  # not yet implemented.
  
    # imports
    from sklearn.preprocessing import MinMaxScaler, StandardScaler
    from sklearn.metrics.pairwise import pairwise_distances

    import pandas as pd
    import numpy as np
    pd.set_option('precision', 20)

    
    # scaling.
    if is_scale:
        if scaling == 'minmax':
            scaler = MinMaxScaler(feature_range = (-1, 1))
            normalized_df = df.set_index('uri')
            normalized_df = scaler.fit_transform(normalized_df)
            normalized_df = pd.DataFrame(normalized_df, columns = [col for col in df.columns if col != 'uri'], index=np.array(df['uri']))
        if scaling == 'standard':
            scaler = StandardScaler()
            normalized_df = df.set_index('uri')
            normalized_df = scaler.fit_transform(normalized_df)
            normalized_df = pd.DataFrame(normalized_df, columns = [col for col in df.columns if col != 'uri'], index=np.array(df['uri']))
    else:
        normalized_df = df.copy()


    # get song uri's.   
    
    df_results = df.copy()
    if is_track_uri:
        # get similarity scores.
        try:
            df_results = df.set_index('uri')
        except:
            a=1
        n_smallest = {}
        for similarity in similarities:
            print(f'{similarity} is in progress..\n')
            df_results[f'{similarity}_distances'] = pairwise_distances(normalized_df.to_numpy(), normalized_df.loc[track_uri].to_frame().T, metric=similarity)
            n_smallest[f'{similarity}_distances'] = df_results[f'{similarity}_distances'].nsmallest(n_recommendations + 1) # this contains the parent itself as the most similar entry, hence n+1 to get n children
        n_smallest = pd.DataFrame(n_smallest)

        if return_df:
            return df_results, n_smallest
        else:
            return n_smallest

    else:
        # get similarity scores.
        df_results = df.copy()
        n_smallest = {}

        for similarity in similarities:
            if verbose: 
                print(f'{similarity} is in progress..\n')
            y_vector=pd.DataFrame(np.array(array).reshape(-1, 13), columns = [col for col in df.columns if col != 'uri'])
            df_results[f'{similarity}_distances'] = pairwise_distances(normalized_df.to_numpy(), y_vector, metric=similarity)
            n_smallest[f'{similarity}_distances'] = df_results[f'{similarity}_distances'].nsmallest(n_recommendations + 1) # this contains the parent itself as the most similar entry, hence n+1 to get n children

        n_smallest = pd.DataFrame(n_smallest)

        if return_df:
            return df_results, n_smallest
        else:
            return n_smallest

def cluster(df, 
            n_clusters,
            pickle_name='not_necessary',
            read_from_pickle=False,
            read_from_pickle_address='/content/drive/MyDrive/BDA_CAPSTONE/02_Model_Pickles/01_KMeans/first_clustering.pkl',
            pickle_address = '/content/drive/MyDrive/BDA_CAPSTONE/02_Model_Pickles/01_KMeans/',
            distances_path = '/content/drive/MyDrive/BDA_CAPSTONE/08_Distances_for_search_results/',
            write_pickle=True,
            center_info=True,
            point_to_center_distance_info=True,
            max_iter=300,
            n_init=20,
            init='k-means++',
            algorithm='lloyd',
            random_state=42):
  
    from sklearn.cluster import KMeans
    import matplotlib.pyplot as plt
    import numpy as np
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix
    from sklearn.model_selection import train_test_split

    import pickle

    import warnings
    warnings.filterwarnings("ignore")

    if not read_from_pickle:
        model = KMeans(n_clusters=n_clusters, 
                      max_iter=max_iter, 
                      n_init=n_init, 
                      init=init, 
                      random_state=random_state)
        model.fit(df)
    else:
        model = read_dump_pickle(path=read_from_pickle_address, read=True, dump=False)
    
    

    predictions = model.predict(df)

    centers = model.cluster_centers_ 
    inertias = model.inertia_  # sum of squared distances

    model_initial_results = {}

    model_initial_results['n_clusters'] = n_clusters
    model_initial_results['max_iter'] = max_iter
    model_initial_results['n_init'] = n_init
    model_initial_results['init'] = init
    model_initial_results['random_state'] = random_state
    model_initial_results['inertias'] = inertias

    if center_info:
        for cluster in range(n_clusters):
            model_initial_results[f'cluster_{cluster}_center'] = centers[cluster]

    df_model_initial_results = pd.DataFrame.from_dict([model_initial_results])

    df_with_clusters = df.copy()
    df_with_clusters['clusters'] = predictions
    df_with_clusters = df_with_clusters.sort_values('clusters', ascending=True)
    numeric_cols = [col for col in df_with_clusters.columns if col != 'clusters']
    distances = []
    if point_to_center_distance_info:
        for cluster in range(n_clusters):
            cluster_centroid = model_initial_results[f'cluster_{cluster}_center']
            cluster_df = df_with_clusters[df_with_clusters['clusters'] == cluster][numeric_cols]
            for point in range(len(cluster_df)):
                point_vector = np.array(cluster_df.iloc[point][numeric_cols])
                dist = np.linalg.norm(point_vector-cluster_centroid)
                distances.append(dist)
        df_with_clusters['distances'] = distances

    if write_pickle:
        pickle.dump(model, open(f"{pickle_address}{pickle_name}.pkl", "wb"))

    df_with_clusters.to_csv(f'{distances_path}df_with_{n_clusters}_clusters_and_distances.csv')

    return df_model_initial_results, df_with_clusters

def clustered_logistic_regression(df_clustered,
                                  results_from_cluster,
                                  train_test_ratio=0.3,
                                  verbose=False):

    import matplotlib.pyplot as plt
    import numpy as np
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix
    from sklearn.model_selection import train_test_split

    import warnings
    warnings.filterwarnings("ignore")

    df_temp = df_clustered.copy()
    number_of_unique_clusters = df_temp['clusters'].nunique()
    unique_clusters = sorted(df_temp['clusters'].unique())

    frames = []

    # add 1 loop
    for cluster_no in unique_clusters:
        print(f'\nCluster number: {cluster_no+1} / {number_of_unique_clusters}')
        df_temp['clusters'] = df_clustered['clusters'].apply(lambda x: 1 if x == cluster_no else 0)
        df_temp2 = df_temp[df_temp['clusters'] == 0]
        df_temp3 = df_temp[df_temp['clusters'] == 1]  # the ones that belong to cluster_no

        # add 1 loop
        xs = np.array_split(df_temp2.to_numpy(), np.round((len(df_temp)/len(df_temp3)), 0))
        total_number_of_folds = len(xs)

        for fold_no in range(total_number_of_folds):
            if verbose:
                print(f'Special fold number: {fold_no+1} / {total_number_of_folds}')

            new_temp_x_df = pd.DataFrame(xs[fold_no], columns=df_temp3.columns)
            new_df = pd.concat([df_temp3, new_temp_x_df], axis=0)

            x = new_df.drop('clusters', axis=1).to_numpy()
            y = new_df['clusters'].tolist()

            X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=train_test_ratio, random_state=42, stratify=y)

            model = LogisticRegression()

            model.fit(X_train, Y_train)

            predictions = model.predict(X_test)
            test_accuracy = model.score(X_test, Y_test)
            train_accuracy = model.score(X_train, Y_train)
            cm = confusion_matrix(Y_test, predictions)

            temp_results = {}
            temp_results[f'train_len'] = X_train.shape[0]
            temp_results[f'test_len'] = X_test.shape[0]
            temp_results[f'number_of_unique_clusters'] = number_of_unique_clusters
            temp_results[f'cluster_no'] = cluster_no
            temp_results[f'fold_no'] = fold_no
            temp_results[f'train_test_ratio'] = train_test_ratio

            
            temp_results[f'train_accuracy'] = train_accuracy
            temp_results[f'test_accuracy'] = test_accuracy
            temp_results[f'train_gini'] = 2*train_accuracy - 1
            temp_results[f'test_gini'] = 2*test_accuracy - 1

            a = cm[0][0]
            b = cm[0][1]
            c = cm[1][0]
            d = cm[1][1]
            temp_results[f'true_0s'] = a
            temp_results[f'false_0s'] = b
            temp_results[f'true_1s'] = d
            temp_results[f'false_1s'] = c

            temp_results[f'0_predictive_power'] = a / (a + b)
            temp_results[f'1_predictive_power(precision)'] = d / (c + d)
            temp_results[f'recall'] = a / (a + c)
            temp_results[f'specificity'] = d / (b + d)
            temp_results[f'f1_score'] = ((d / (c + d)) * (a / (a + c)) * 2) / ((d / (c + d)) + (a / (a + c)))

            temp_results_df = pd.DataFrame(temp_results, index=[0])

            frames.append(temp_results_df)
  
    all_results = pd.concat(frames, axis=0, ignore_index=True)
    results_from_cluster = pd.concat([results_from_cluster]*len(all_results), ignore_index=True)
    all_results = pd.concat([all_results, results_from_cluster], axis=1)

    return all_results

def connect_Spotify(client_id,
                    client_secret,
                    playlist_creator_mode=False):

    client_id=client_id
    client_secret=client_secret

    if playlist_creator_mode:
        sp = spotipy.Spotify(auth_manager=SpotifyOAuth(client_id=client_id, 
                                                    client_secret= client_secret,
                                                  redirect_uri='https://stackoverflow.com/',
                                                  open_browser=False,
                                                  scope="playlist-modify-private"))
    else:
        sp = spotipy.Spotify(auth_manager=SpotifyOAuth(client_id=client_id, 
                                                    client_secret= client_secret,
                                                  redirect_uri='https://stackoverflow.com/',
                                                  open_browser=False))
    
    return sp

class ContentBasedRecommender:

    def __init__(self, connection):
        self.connection = connection
        
    
    ## PART-1 : Retrieve tracks
    def retrieve_track_features(self, tracks):
        self.tracks = tracks

        track_features, _ = retrieve_tracks_and_features(all_unique_tracks=tracks,
                                                                connection=self.connection)
        
        return track_features


    def clean_results(self, combined_input_results, scale=True):
        self.combined_input_results = combined_input_results
        self.scale = scale

        import pandas as pd
        import numpy as np
        from sklearn.preprocessing import MinMaxScaler

        cols_to_keep = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms', 'time_signature']
        uri_col_name = 'uri'

        combined_input_results = combined_input_results.set_index(uri_col_name)[cols_to_keep]

        # if scale:
        #     mms = MinMaxScaler()
        #     combined_input_results_scaled = mms.fit_transform(combined_input_results)
        #     combined_input_results_scaled = pd.DataFrame(combined_input_results_scaled, columns=combined_input_results.columns, index=combined_input_results.index)
        if scale:
            combined_input_results = self.scale_clustered_results(combined_input_results)
            # combined_input_results.set_index('uri', inplace=True)

        return combined_input_results

    def scale_clustered_results(self, search_results_raw):
        self.search_results_raw = search_results_raw
        
        search_uris = search_results_raw.reset_index()['uri'].tolist()
        df_raw = load_track_features(scaled=False)
        df_all_raw = pd.concat([df_raw, search_results_raw.reset_index()], axis=0).set_index('uri')

        from sklearn.preprocessing import MinMaxScaler
        mms = MinMaxScaler()
        df_scaled = mms.fit_transform(df_all_raw)
        df_scaled = pd.DataFrame(df_scaled, columns=df_all_raw.columns, index=df_all_raw.index)

        search_results_scaled = df_scaled.query(f'uri in {search_uris}')
        # df_scaled_excpt_for_search = df_scaled.query(f'uri not in {search_uris}')
        return search_results_scaled

    def search(self, name, category, limit, return_track_features=True, clean_results=True):
        self.name = name
        self.category = category
        self.limit = limit
        self.return_track_features = return_track_features
        
        if not return_track_features:
            if category == 'album':
                album_uri = self.connection.search(name, type='album', limit=limit)['albums']['items'][0]['uri']
                album_tracks = [self.connection.album_tracks(album_uri)['items'][track_counter]['uri'] for track_counter in range(len(self.connection.album_tracks(album_uri)['items']))]
                return album_tracks

            elif category == 'track':
                song = self.connection.search(name, type='track', limit=limit)['tracks']['items'][0]['uri']
                return song

            elif category == 'artist':
                artist_uri = self.connection.search(name, type='artist', limit=limit)['artists']['items'][0]['uri']
                artist_top_n_songs = [self.connection.artist_top_tracks(artist_uri)['tracks'][track_counter]['uri'] for track_counter in range(limit)]
                return artist_top_n_songs

            else:
                raise Exception('Please check entered category type. It should be either "album", "track" or "artist"')
        
        else:  # return track features by default
            if category == 'album':
                album_uri = self.connection.search(name, type='album', limit=limit)['albums']['items'][0]['uri']
                album_tracks = [self.connection.album_tracks(album_uri)['items'][track_counter]['uri'] for track_counter in range(len(self.connection.album_tracks(album_uri)['items']))]
                track_features = self.retrieve_track_features(album_tracks)
                if clean_results:
                    track_features = self.clean_results(track_features, scale=True)
                # track_features = track_features.mean().to_frame().T
                # track_features.index.name = 'uri'
                track_features = track_features.drop_duplicates()
                return track_features

            elif category == 'track':
                song = self.connection.search(name, type='track', limit=limit)['tracks']['items'][0]['uri']
                track_features = self.retrieve_track_features(song)
                if clean_results:
                    track_features = self.clean_results(track_features, scale=True)
                # track_features = track_features.mean().to_frame().T
                # track_features.index.name = 'uri'
                track_features = track_features.drop_duplicates()
                return track_features

            elif category == 'artist':
                artist_uri = self.connection.search(name, type='artist', limit=limit)['artists']['items'][0]['uri']
                artist_top_n_songs = [self.connection.artist_top_tracks(artist_uri)['tracks'][track_counter]['uri'] for track_counter in range(limit)]
                track_features = self.retrieve_track_features(artist_top_n_songs)
                if clean_results:
                    track_features = self.clean_results(track_features, scale=True)
                # track_features = track_features.mean().to_frame().T
                # track_features.index.name = 'uri'
                track_features = track_features.drop_duplicates()
                return track_features

            else:
                raise Exception('Please check entered category type. It should be either "album", "track" or "artist"')
        

      
    ## PART-2 : Cluster results
    def read_dump_pickle(self, path, read=True, dump=False, dump_object=None):
        self.path = path
        self.read = read
        self.dump = dump
        self.dump_object = dump_object

        import pickle

        if read:
            with open(path, 'rb') as handle:
                model = pickle.load(handle)
                print(f"Pickle read from {path}.")
            return model
        elif dump:
            with open(path, 'wb') as handle:
                pickle.dump(dump_object, handle, protocol=pickle.HIGHEST_PROTOCOL)
            print(f"Pickle dumped under {path}.")
        else:
            print("Something is wrong with the arguments.")



    def load_clustered_dataset(self, clustered_dataset_path='/content/drive/MyDrive/BDA_CAPSTONE/01_Gathering_Data/03_Processed_Data/df_scaled_clustered.csv'):
        self.clustered_dataset_path = clustered_dataset_path

        clustered_dataset = pd.read_csv(clustered_dataset_path).set_index(['uri', 'first_clusters', 'second_clusters'])

        return clustered_dataset







    def cluster_search_results(self, results_scaled, cluster_path='/content/drive/MyDrive/BDA_CAPSTONE/02_Model_Pickles/01_KMeans/'):
        self.results_scaled = results_scaled  # output of search
        self.cluster_path = cluster_path

        all_frames = []

        import glob
        first_cluster_path = glob.glob(f'{cluster_path}first_clustering.pkl')[0]

        _, results_scaled_with_clusters = cluster(results_scaled,
                                                  n_clusters=50,
                                                  read_from_pickle=True,
                                                  read_from_pickle_address=first_cluster_path,
                                                  write_pickle=False)
        results_scaled_with_clusters = results_scaled_with_clusters.rename(columns={'clusters': 'first_clusters'}).set_index('first_clusters', append=True).drop('distances', axis=1)

        for m_clusters in range(50):
            path = f'{cluster_path}second_clustering_{m_clusters}.pkl'

            temp_df = results_scaled_with_clusters.query(f'first_clusters == {m_clusters}')
            if len(temp_df) == 0:
                pass
            else:
                print(f'Cluster number {m_clusters} out of 50 is being clustered...')
                results, df_clusters_with_clusters = cluster(temp_df,
                                                              n_clusters=50,
                                                              read_from_pickle=True,
                                                              read_from_pickle_address=path,
                                                              write_pickle=False)
                
                all_frames.append(df_clusters_with_clusters)


        df_all_clusters = pd.concat(all_frames, axis=0)
        df_all_clusters.rename(columns={'clusters': 'second_clusters'}, inplace=True)

        df_all_clusters = df_all_clusters.set_index('second_clusters', append=True)
        df_all_clusters = df_all_clusters.drop('distances', axis=1)

        return df_all_clusters



    ## PART-3 : Recommend
    def content_based_recommender(self, search_results_clustered, n_recoms=50, return_n_recommendations=20, use_clusters=True):
        self.search_results_clustered = search_results_clustered  # output of cluster_search_results
        self.n_recoms = n_recoms
        self.return_n_recommendations = return_n_recommendations
        self.use_clusters = use_clusters

        df_scaled_clustered = self.load_clustered_dataset()


        ## USING CLUSTERS
        if use_clusters:
            search_recommendations = {}


            for row in range(len(search_results_clustered)):
                print(row, len(search_results_clustered))
                first_cluster = search_results_clustered.reset_index()['first_clusters'][row]
                second_cluster = search_results_clustered.reset_index()['second_clusters'][row]

                search_results_clustered.index.name = 'uri'
                temp_df = search_results_clustered.iloc[row:row+1]
                track_name = temp_df.reset_index()['uri'].tolist()[0]

                df_scaled_clustered_filtered = df_scaled_clustered.query(f'(first_clusters == {first_cluster}) and (second_clusters == {second_cluster})')
                recommendations, n_smallest = content_based_recommend(df=df_scaled_clustered_filtered,
                                                                      array=temp_df,  # 13 elements
                                                                      track_uri='spotify:track:75L6ZI3hbF3vbkOyBlJup8',
                                                                      is_track_uri=False,
                                                                      n_recommendations=n_recoms,
                                                                      similarities=['cosine', 'l1', 'l2', 'chebyshev'],  # more will be added soon. 'mahalanobis', 'seuclidean' are problematic.
                                                                      is_scale=False,
                                                                      scaling='minmax',
                                                                      concat_distances=False,  # not yet implemented.
                                                                      return_df=True,
                                                                      write_results=True)  # not yet implemented.

                recommendations = n_smallest.sort_values(by=['cosine_distances', 'l1_distances', 'l2_distances', 'chebyshev_distances'], ascending=True)
                recommendations = [recommendations.index[track_counter][0] for track_counter in range(len(recommendations))]
                search_recommendations[track_name] = recommendations[1:]

            keys = search_recommendations.keys()

            exit_flag = False
            for n_track_counter in range(1, 1000):  # 100 is arbitrary, but should be plenty enough
                if exit_flag:
                    break
                first_n_tracks_from_all_keys = []  
                for key in keys:
                    first_n_tracks_from_all_keys.extend(search_recommendations[key][:n_track_counter])
                    first_n_tracks_from_all_keys = np.array(first_n_tracks_from_all_keys).flatten().tolist()
                    if len(set(first_n_tracks_from_all_keys)) >= return_n_recommendations:  # 20 is arbitrary
                        exit_flag = True
                        break

            recommendation_list = [search_recommendations[key][:n_track_counter] for key in keys]
            recommendation_list = np.array(recommendation_list).flatten().tolist()[:return_n_recommendations]
        
            return recommendation_list
        ## NOT USING CLUSTERS
        else:
            df_scaled = load_track_features(scaled=True)
            if 'uri' in df_scaled.columns:
                df_scaled = df_scaled.set_index('uri')

            search_recommendations = {}
            for row in range(len(search_results_clustered)):
                print(row, len(search_results_clustered))
                # first_cluster = search_results_clustered.reset_index()['first_clusters'][row]
                # second_cluster = search_results_clustered.reset_index()['second_clusters'][row]

                search_results_clustered.index.name = 'uri'
                temp_df = search_results_clustered.iloc[row:row+1]
                track_name = temp_df.reset_index()['uri'].tolist()[0]

                # df_scaled_clustered_filtered = df_scaled_clustered.query(f'(first_clusters == {first_cluster}) and (second_clusters == {second_cluster})')
                recommendations, n_smallest = content_based_recommend(df=df_scaled,
                                                                      array=temp_df,  # 13 elements
                                                                      track_uri='spotify:track:75L6ZI3hbF3vbkOyBlJup8',
                                                                      is_track_uri=False,
                                                                      n_recommendations=50,
                                                                      similarities=['cosine', 'l1', 'l2', 'chebyshev'],  # more will be added soon. 'mahalanobis', 'seuclidean' are problematic.
                                                                      is_scale=False,
                                                                      scaling='minmax',
                                                                      concat_distances=False,  # not yet implemented.
                                                                      return_df=True,
                                                                      write_results=True)  # not yet implemented.

                recommendations = n_smallest.sort_values(by=['cosine_distances', 'l1_distances', 'l2_distances', 'chebyshev_distances'], ascending=True)
                recommendations = recommendations.index.tolist()
                search_recommendations[track_name] = recommendations[1:]




            keys = search_recommendations.keys()

            exit_flag = False
            for n_track_counter in range(1, 1000):  # 100 is arbitrary, but should be plenty enough
                if exit_flag:
                    break
                first_n_tracks_from_all_keys = []  
                for key in keys:
                    first_n_tracks_from_all_keys.extend(search_recommendations[key][:n_track_counter])
                    first_n_tracks_from_all_keys = np.array(first_n_tracks_from_all_keys).flatten().tolist()
                if len(set(first_n_tracks_from_all_keys)) >= return_n_recommendations:  # 20 is arbitrary
                    exit_flag = True
                    break

            recommendation_list = [search_recommendations[key][:n_track_counter] for key in keys]
            recommendation_list = np.array(recommendation_list).flatten().tolist()[:return_n_recommendations]

            return recommendation_list



    def get_unique_N(iterable, N):
        """Yields (in order) the first N unique elements of iterable. 
        Might yield less if data too short."""
        seen = set()
        for e in iterable:
            if e in seen:
                continue
            seen.add(e)
            yield e
            if len(seen) == N:
                return



def create_playlist(sp,
                    playlist_name,
                    search_term,
                    search_term_type,
                    search_term_limit,
                    use_clusters=True,
                    return_n_recommendations=10,
                    return_qrcode=True):
    import qrcode
    results = sp.current_user()
    user_id = results['id']

    playlist = sp.user_playlist_create(user_id, playlist_name, public=False)
    playlist_id = playlist['uri']
    cbr = ContentBasedRecommender(connection=sp)
    print(f'{playlist_name} playlist has been created.')

    tracks = cbr.search(name=search_term, category=search_term_type, limit=search_term_limit, return_track_features=True, clean_results=True)
    print(f'Tracks have been gathered for the search term {search_term} under the category of {search_term_type}.')

    clustered_search_results = cbr.cluster_search_results(tracks)
    print(f'Tracks have been clustered.')

    print(f'Recommending to each gathered track...')
    recommendations = cbr.content_based_recommender(clustered_search_results, use_clusters=True, return_n_recommendations=return_n_recommendations)

    sp.user_playlist_add_tracks(user_id, playlist_id, recommendations)
    print(f'Recommended tracks are added to the playlist {playlist_name}.')

    if return_qrcode:
        print('Recommended tracks have been added to the playlist. Use the QR Code to go to the playlist.')
        img = qrcode.make(f'https://open.spotify.com/playlist/{playlist_id.split(":")[-1]}')
        return img
    else:
        print('Recommended tracks have been added to the playlist.')

